{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction to CNN - Exercise\n",
    "---\n",
    "Instructions are given in <span style=\"color:blue\">blue</span> color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you are asked to implement a convolutional neural network based on a proposed architecture to classify objects in the [worlds largest doodling dataset](https://quickdraw.withgoogle.com/data).\n",
    "\n",
    "```Quick, Draw!``` is a project by Google in which people are asked to draw images of certain objects in a limited amount of time. You can actually contribute to the project yourself by visiting [this page](https://quickdraw.withgoogle.com/) and start doodling away. Can you muster the artistic skills it takes to get your images recognized?\n",
    "\n",
    "The entire dataset contains more than 50 million drawings across more than 300 different categories. Luckily, the whole dataset [has been made open-source](https://github.com/googlecreativelab/quickdraw-dataset), allowing us to use the data for our own neural network training.\n",
    "\n",
    "Since trying to classify the entire dataset in a single exercise would be a little overwhelming, we will be focusing on a subset of the drawings instead. However, we still want to tackle a little challenge. This is why we will be working with doodles that are not always as easily distinguishable - not even for the human eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The categories we want to classify\n",
    "classes = ['Apple', 'Baseball', 'Basketball', 'Blueberry', 'Circle', 'Clock', 'Cookie', 'Donut', 'Face', 'Pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Packages and libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "# force gpu computing, when gpu library is available\n",
    "USE_GPU = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<span style=\"color:blue\">As in the material notebook, please use the helper functions for the training loop</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions for the training loop\n",
    "from train_loop import train, train_for_epochs, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data source path\n",
    "source_path = \"data/QD_Data/\"\n",
    "\n",
    "# Images and labels\n",
    "training_img_file = \"QD_Training_Data.npy\"\n",
    "validation_img_file = \"QD_Validation_Data.npy\"\n",
    "testing_img_file = \"QD_Testing_Data.npy\"\n",
    "training_label_file = \"QD_Training_Labels.npy\"\n",
    "validation_label_file = \"QD_Validation_Labels.npy\"\n",
    "testing_label_file = \"QD_Testing_Labels.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* First, we get the data ready to work with on your local machine. You can download the data from this course's Moodle page. In total, six files have been prepared for you: the training data, the validation data, and the testing data along with their respective labels. All files are stored in the binary <code>.npy</code> format. The content of these files is loaded into your workspace with the help of NumPy's <code>load()</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_img = np.load(f'{source_path}{training_img_file}')\n",
    "validation_img = np.load(f'{source_path}{validation_img_file}')\n",
    "testing_img = np.load(f'{source_path}{testing_img_file}')\n",
    "\n",
    "training_label = np.load(f'{source_path}{training_label_file}')\n",
    "validation_label = np.load(f'{source_path}{validation_label_file}')\n",
    "testing_label = np.load(f'{source_path}{testing_label_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* Numer of samples for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'There are {len(training_img)} samples for training, {len(validation_img)} for validation, and {len(testing_img)} for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Are the classes balanced? Let us see out how many samples there are for each category in the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Distribution of training data')\n",
    "n, bins, patches = plt.hist(training_label)\n",
    "plt.show()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Distribution of validation data')\n",
    "n, bins, patches = plt.hist(validation_label)\n",
    "plt.show()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Distribution of test data')\n",
    "n, bins, patches = plt.hist(testing_label)\n",
    "plt.show()\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Optimally, those results should help you to conclude that there is plenty of data to train with, and the classes are all very well balanced. However, this is usually (and unfortunately) seldom the case. Try to think of a situation where there are not enough images to train a network.\n",
    "\n",
    "One possible method to tackle this issue is found in **Data Augmentation**.\n",
    "\n",
    "* <div style=\"color:blue\">Please describe, in your own words, what Data Augmentation is and how it can be used to increase model performance.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "*Your solution (statement) goes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* Let us visualize the first image of the training set in conjunction with its correct label (category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(training_img[0])\n",
    "plt.title(classes[int(training_label[0])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Now let us have a look at the first 25 images of the testing set in conjunction with their correct labels (categories). Are you able to accurately identify each doodle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "for i in range(25):\n",
    "    label = classes[int(testing_label[i])]\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    img = testing_img[i]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 3. Preprocess the data and build a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In terms of preprocessing the data, there is not much left for you to do. The provided image data has a similar format as the MNIST data you have seen several times by now.\n",
    "\n",
    "* Images are represented by $28x28$ grayscale pixels.\n",
    "* Each pixel holds a value between $0$ and $255$.\n",
    "\n",
    "Before designing and training your network, you have to build a dataset and make sure those pixel values are scaled between 0 and 1.\n",
    "\n",
    "* <div style=\"color:blue\">Build a PyTorch dataset and instantiate it for the training, validation, and test data</div>\n",
    "\n",
    "* <div style=\"color:blue\">Scale your data's pixel values to the range <b>[0, 1]</b>.</div>\n",
    "\n",
    "**Hint**: Make sure to perform all preprocessing tasks on your training as well as your validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4. Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that all the data is available and adequately preprocessed, it is time to create your convolutional neural network. Typically, the process of designing the model can take quite a while as there is no single best CNN architecture, taking into account the vast amount of hyperparameters and model options. Thus, finding a network structure that perfectly suits your data (and use-case) is a considerable effort on its own.\n",
    "\n",
    "* <div style=\"color:blue\">For this exercise, however, you will not design your own network. Instead, you are asked to implement the following CNN (using <code>PyTorch</code>):</div>\n",
    "\n",
    "<img src=\"img/Ex_CNN_Model_Visualization.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* <div style=\"color:blue\">Each convolutional layer applies a 3x3 filter with a 1x1 stride. The images will be padded so that the convolved feature maps retain the <b>same</b> size. The number of filters (kernels) should be taken from the visualization above.</div>\n",
    "* <div style=\"color:blue\">Subsampling is achieved by using Max-Pooling with a pool size of 2x2 (for each pooling layer).</div>\n",
    "* <div style=\"color:blue\">Apart from the final dense layer, activation happens with the help of the Rectified Linear Unit (ReLU). The last layer should output a probability distribution of the categories.</div>\n",
    "* <div style=\"color:blue\">After the first dense layer, dropout regularization is performed with a dropout rate of 0.5.</div>\n",
    "\n",
    "**Hint**: Printing an instance of your model object will print an overview of your current model architecture. The CNN you are asked to implement should produce the following output:\n",
    "\n",
    "<img src=\"img/Ex_CNN_Model_Summary_PYTORCH.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5. Setup dataloader and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <div style=\"color:blue\">After defining the model architecture, you can now create a dataloader for the train and test dataset, setup an optimizer, and define a loss function. Use the Following:</div>\n",
    "\n",
    "    * **Optimizer**: Adam\n",
    "    * **Loss-Function**: torch.nn.CrossEntropyLoss\n",
    "    * **Metrics**: Accuracy\n",
    "   \n",
    "Try to find a suitable value for the batch size (the number of images fed into the network at once). <br>\n",
    "Also move the model to the correct device by calling <code>model.to(device)</code> on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 6. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <div style=\"color:blue\">Now, train (fit) your model on the training data. Find suitable values for the number of epochs. (It is actually possible to break the 90% accuracy barrier.)</div>\n",
    "<span style=\"color:blue\">As in the material notebook, please use the helper functions for the training loop (see the imports above)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:\n",
    "# history = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let us visualize how the accuracy and the loss values for both testing and training data have evolved during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_train_hist = history['train']['loss']\n",
    "acc_train_hist = history['train']['accuracy']\n",
    "loss_val_hist = history['val']['loss']\n",
    "acc_val_hist = history['val']['accuracy']\n",
    "\n",
    "x_arr = np.arange(len(loss_train_hist)) + 1\n",
    "\n",
    "with plt.ioff():\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, loss_train_hist, '-o', label='Train loss')\n",
    "    ax.plot(x_arr, loss_val_hist, '--<', label='Validation loss')\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Loss', size=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x_arr, acc_train_hist, '-o', label='Train acc.')\n",
    "    ax.plot(x_arr, acc_val_hist, '--<', label='Validation acc.')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Accuracy', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = validate(device, model, test_loader, criterion, binary=False)\n",
    "print('\\nTest Acc. {:.2f}%'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 7. Using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* Let us finally put the model to use and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for predicting the class at a given index by a trained model\n",
    "def predict(model, data, binary=True):\n",
    "    model.eval()\n",
    "    data = data.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        #predict\n",
    "        outputs = model(data).squeeze().cpu()\n",
    "        # Get the prediction by selecting the class with the highest probability\n",
    "        outputs = torch.softmax(outputs, 0)\n",
    "        _, predicted = torch.max(outputs, 0)\n",
    "        return predicted, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example for test image at index 883\n",
    "idx = 883\n",
    "sample, label = test_dataset[idx]\n",
    "img = testing_img[idx]\n",
    "pred, probs = predict(model, sample, binary=False)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(f'GT: {classes[label]}, Pred: {classes[pred]}, Prob: {probs[pred] * 100}%')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* <div style=\"color:blue\">Visualize the entire probability distribution for the prediction of that same image sample. What is your network's second-best guess?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "*Your solution (statement) goes here:* The network gets it wrong: it thinks, this is a *Blueberry*, even though a *Baseball* is depicted. However, the network's second best guess would be *Baseball*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example for test image at index 22866\n",
    "idx = 22866\n",
    "sample, label = test_dataset[idx]\n",
    "img = testing_img[idx]\n",
    "pred, probs = predict(model, sample, binary=False)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(f'GT: {classes[label]}, Pred: {classes[pred]}, Prob: {probs[pred] * 100}%')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* <div style=\"color:blue\">Visualize the entire probability distribution for the prediction of that same image sample. What are your thoughts?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "*Your solution (statement) goes here:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
