{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38922051",
   "metadata": {},
   "source": [
    "Homework Group A\n",
    "## Osama Al Kamel / Mtr Num: 3141575\n",
    "## Joshua Oldridge / Mtr Num: 3140770\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95f9c9-a791-49fa-a3d6-710d5a12dc5c",
   "metadata": {},
   "source": [
    "# Introduction to Vision Transformers - Mini Project\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d533a-989b-4a36-85b1-c613d5dd6ab9",
   "metadata": {},
   "source": [
    "Instructions are given in <span style=\"color:blue\">blue</span> color.\n",
    "\n",
    "**General Instructions and Hints**\n",
    "\n",
    "* <span style=\"color:blue\"> In your solution notebook, make it clear and explain what you did for which one of the tasks using markdown and / or commentary as appropriate.</span>\n",
    "* You will be able to make use of or at least be inspired by some of the material already provided for other topics in this class\n",
    "* <span style=\"color:red\"> Whenever you use something from a specific source or by employing a specific tool <b>academic honesty demands</b> that you reference the original source!!!</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acd69e-4b9b-4f31-99d6-8e0c5d5583e4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- [Imports](#Imports)\n",
    "- [Task 1: Inductive bias in transformer models](#Task-1:-Inductive-bias-in-transformer-models)\n",
    "- [Task 2: Visualizing attention in ViTs](#Task-2:-Visualizing-attention-in-ViTs)\n",
    "  - [Task 2.1: Modify our ViT implementation to output the attention matrix](#Task-2.1:-Modify-our-ViT-implementation-to-output-the-attention-matrix)\n",
    "  - [Task 2.2: Write a function that creates an attention heatmap for a specific image](#Task-2.2:-Write-a-function-that-creates-an-attention-heatmap-for-a-specific-image)\n",
    "  - [Task 2.3: Use the function to visualize attention maps for example images](#Task-2.3:-Use-the-function-to-visualize-attention-maps-for-example-images)\n",
    "- [Task 3: Using the inductive bias of CNNs to support the training of ViTs](#Task-3:-Using-the-inductive-bias-of-CNNs-to-support-the-training-of-ViTs)\n",
    "  - [Task 3.1: Training a teacher model](#Task-3.1:-Training-a-teacher-model)\n",
    "  - [Task 3.2: Distillation loss](#Task-3.2:-Distillation-loss)\n",
    "  - [Task 3.3: Changing the ViT architecture to allow distillation](#Task-3.3:-Changing-the-ViT-architecture-to-allow-distillation)\n",
    "  - [Task 3.4: Train your own DeiT model](#Task-3.4:-Train-your-own-DeiT-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad417d1e-488a-4dbf-bf40-9a06491c156e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c2c774-1f3c-43ab-9b96-d3254ecf4106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "cuda available: True, using cuda, GPU name: NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "print(torch.__version__)\n",
    "#check if cuda is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "#Added device name printing and device variable\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "print(f\"cuda available: {cuda_available}, using {device}, GPU name: {device_name}\")\n",
    "use_cuda = cuda_available\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1184a73-a9a9-467c-ad38-d0213dd08692",
   "metadata": {},
   "source": [
    "We can keep the same configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae04d3-d9e3-4bbd-94b4-9448d3e41816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "WARMUP_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "N_WORKERS = 0\n",
    "LR = 5e-4\n",
    "OUTPUT_PATH = './outputs'\n",
    "\n",
    "# Data parameters\n",
    "DATASET = 'fmnist'\n",
    "IMAGE_SIZE = 28\n",
    "PATCH_SIZE = 4\n",
    "N_CHANNELS = 1\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "# ViT parameters\n",
    "EMBED_DIM = 64\n",
    "N_ATTENTION_HEADS = 4\n",
    "FORWARD_MUL = 2\n",
    "N_LAYERS = 6\n",
    "DROPOUT = 0.1\n",
    "MODEL_PATH = './model'\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc71a6c-0e65-4497-abcd-fcb3d12f430e",
   "metadata": {},
   "source": [
    "In this miniproject you will use the **same dataset for your experiments** as in the material notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60b6fc-3b99-4d7a-bdd0-3e7622091ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize([IMAGE_SIZE, IMAGE_SIZE]),\n",
    "                              transforms.RandomCrop(IMAGE_SIZE, padding=2), \n",
    "                              transforms.RandomHorizontalFlip(),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize([0.5], [0.5])])\n",
    "train = datasets.FashionMNIST(os.path.join(DATA_PATH, DATASET), train=True, download=True, transform=train_transform)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize([IMAGE_SIZE, IMAGE_SIZE]), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "test = datasets.FashionMNIST(os.path.join(DATA_PATH, DATASET), train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=N_WORKERS,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=N_WORKERS,\n",
    "                                            drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104c0a6-5b7a-46d2-b052-704a4cf6de44",
   "metadata": {},
   "source": [
    "## Task 1: Inductive bias in transformer models\n",
    "\n",
    "In the first task of this miniproject you should develop a deeper understanding of why transformer models function well on various data structures and large data domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be92e17-0540-48e3-b842-a62968e82fe3",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    Watch the <a href=\"https://www.youtube.com/watch?v=TrdevFK_am4\">video</a> of Yannic Kilcher about the vision transformer <a href=\"https://arxiv.org/pdf/2010.11929\">paper</a> and answer the following questions:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9d0d9-fd24-4c8f-87c6-c6957f95abee",
   "metadata": {},
   "source": [
    "* What is the biggest weakness of transformer models from a complexity point of view and why can the use of image patches for vision transformers help with that?\n",
    "<div style=\"color:lightblue\"> </div>\n",
    "\n",
    "* What is meant by the term \"inductive bias\" (sometimes he calls it \"inductive prior\")?\n",
    "<div style=\"color:lightblue\"> </div>\n",
    "\n",
    "* What is the interplay between model bias and the amount of available data?\n",
    "<div style=\"color:lightblue\"> </div>\n",
    "\n",
    "* If skip connections introduce an inductive bias, why are they needed in the transformer model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424295c5",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "<div style=\"color:lightblue\">\n",
    "Before moving on to task 2, lets define the model, its parameters, and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e8fba-306f-4c7f-95f4-3c574c7c64c5",
   "metadata": {},
   "source": [
    "## Task 2: Visualizing attention in ViTs\n",
    "\n",
    "Attention in ViTs tells us which parts of an image are important for some other image parts (or for the classification itself). This provides a form of inherent explainability. In this task, you should explore this option and visualize attention maps to understand, what a ViT is looking at in the image. As a starting point, **read this excellent [blog post](https://jacobgil.github.io/deeplearning/vision-transformer-explainability) by Jacob Gildenblat** to get an understanding of how attention can be visualized in ViTs. Here are some more **implementations** that might help you with the task:\n",
    "\n",
    "* https://github.com/mashaan14/VisionTransformer-MNIST/blob/main/VisionTransformer_MNIST.ipynb\n",
    "* https://github.com/jacobgil/vit-explain/tree/main\n",
    "* https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e66ea0-a4a4-4061-bc7c-6b5f828eecad",
   "metadata": {},
   "source": [
    "### Task 2.1: Modify our ViT implementation to output the attention matrix \n",
    "<span style=\"color:blue\">\n",
    "    To visualize attention for a specific image, the model needs to output not only its prediction, but also the attention matrix. You should adapt the <code>VisionTransformer</code> class that we used in the material notebook in such a way that it outputs the attention matrix <code>x_attention</code>.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf7f6f-5232-4b8f-a841-04773a6bd5b4",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "* The attention map `x_attention` is created inside the `SelfAttention` class\n",
    "* Since `SelfAttention` is used in the `Encoder` and `VisionTransformer` classes, they need to be adapted as well\n",
    "* The model should not output the attention maps everytime it is called, but only if we need them. Implement a `return_attention=False` parameter into the respective `forward()` functions to make this output conditional\n",
    "* Our model uses 4 attention heads and 6 encoder blocks. Furthermore, the model uses 49 image patches plus 1 class token. This means your attention map should have a shape of `[6,4,50,50]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c73d35-2bba-4c82-ab35-f6419a316903",
   "metadata": {},
   "source": [
    "### Task 2.2: Write a function that creates an attention heatmap for a specific image\n",
    "To visualize the attention values as an image, the matrix must first be transformed. Remember, the attention matrix is of shape `[layers, attention_heads, num_patches+class_token, num_patches+class_token]`, but we want to view it as a grayscale image of size 28x28. Therefore, the function needs the following components:\n",
    "\n",
    "* <span style=\"color:blue\">Aggregate the attention weights across all heads. Just like in the <a href=\"https://jacobgil.github.io/deeplearning/vision-transformer-explainability\">blog post</a> by Jacob Gildenblat you should implement a <code>mean</code>, <code>min</code>, and <code>max</code> aggregation! </span>\n",
    "* <span style=\"color:blue\">Again leaning on the idea by <a href=\"https://jacobgil.github.io/deeplearning/vision-transformer-explainability\">Jacob Gildenblat</a>, you should implement a filter to discard attention values below a certain threshold. You can implement this as a function parameter <code>discard_ratio</code>.  </span>\n",
    "* <span style=\"color:blue\">The function should be able to <b>select specific layers</b>. With that we can later see, how different network depths behave.</span>\n",
    "* <span style=\"color:blue\">The function needs to account for <b>residual connections</b> by adding an identity matrix and then re-normlize the weights.</span>\n",
    "* <span style=\"color:blue\">Since we are not interested in the absolut attention values as much as the flow and change of attention weights through the network, you need to <b>recursively multiply the attention weight matrices</b> from successive layers to trace how attention flows from the input to the output.</span>\n",
    "* <span style=\"color:blue\">Finally, you need to reshape the resulting <code>joint_attentions</code> to match the image size.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd3f9e-2abc-44cb-a950-d69e02e71a94",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "* Use the existing implementations that where mentioned before, if you need guidance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455d456-075f-44db-99dc-aaa1c8810504",
   "metadata": {},
   "source": [
    "### Task 2.3: Use the function to visualize attention maps for example images\n",
    "Now you should finally put everything together and visualize some attention maps to understand, what the model is looking at in the images! \n",
    "\n",
    "* <span style=\"color:blue\">Load the weights of the best model from the material notebook <code>ViT_model.pt</code> into your adapted <code>VisionTransformer</code></span>\n",
    "* <span style=\"color:blue\">Visualize multiple original images as well as their attention maps in a grid. </span>\n",
    "* <span style=\"color:blue\">Try different <b>aggregation methods</b> for attention heads, <b>discard ratios</b> and <b>layers</b>.  </span>\n",
    "* <span style=\"color:blue\">Interpret your findings!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ffa1e-ed3a-49a4-bbc5-af5f1ef6b100",
   "metadata": {},
   "source": [
    "If everything works, it should look like this:\n",
    "\n",
    "<img src=\"./img/attention_example.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e5653-f904-4f2e-9545-8120cb8d9216",
   "metadata": {},
   "source": [
    "This specific configuration shows some sensible attention maps. Interestingly, the later layers seem to somewhat inverse the attention of the earlier layers. More experiments and interpretations are expected here! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fd663-b982-4373-82ad-11e4bf24ef31",
   "metadata": {},
   "source": [
    "## Task 3: Using the inductive bias of CNNs to support the training of ViTs\n",
    "As task 1 has shown, the lack of an inductive bias can be a blessing and a curse. While such models excel on huge datasets, it is especially problematic on smaller datasets. In this task you should combine the a CNN and ViT technology to create *the best of two worlds*.\n",
    "\n",
    "In the 2021 paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877), Meta AI did exactly that: combining CNN and ViT to achieve a very high performing model with good convergence properties. This was done by using [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation). A CNN model was trained first and used as a teacher for a ViT student model. With the teachers guidance, the ViT student achieves better performance with shorter training times. **This is exactly what you should be doing in this task!** It is therefore strongly advised that you read the [original paper](https://arxiv.org/pdf/2012.12877). The [paper on knowledge distillation](https://arxiv.org/pdf/1503.02531) by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean could also help to understand this technology!\n",
    "\n",
    "Here are some more **implementations** that might help you with the task:\n",
    "\n",
    "* [DeiT implementation of Francesco Saverio](https://github.com/FrancescoSaverioZuppichini/DeiT)\n",
    "* [DeiT implementation of Shuqi Huang](https://github.com/jiaowoguanren0615/Deit-Pytorch/blob/main/DeiT/models/deitv1.py)\n",
    "* [Official DeiT implementation by Meta](https://github.com/facebookresearch/deit/blob/main/models_v2.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1b4d2-c674-422a-ac6e-9170666ffad0",
   "metadata": {},
   "source": [
    "### Task 3.1: Training a teacher model\n",
    "To distill knowledge from a strong teacher model, you first need to train such a model!\n",
    "\n",
    "* <span style=\"color:blue\">Set up a standard CNN model in PyTorch to train on the fashion mnist dataset.</span>\n",
    "* <span style=\"color:blue\">You can use the CNN from the introduction to CNN models. It should already have the correct layer formats, since we originally used it on the mnist dataset.</span>\n",
    "* <span style=\"color:blue\">Use any training loop to train and evaluate the CNN model. How does it compare to the ViT from the material notebook? <i>Hint: it needs to be better to actually work as a teacher model!</i> </span>\n",
    "* <span style=\"color:blue\">Save the model weights for later use in <code>teacher_model.pt</code></span>\n",
    "* <span style=\"color:blue\">Interpret your findings!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44574691-f635-4b6a-b6fa-60a8f1dc2206",
   "metadata": {},
   "source": [
    "### Task 3.2: Distillation loss\n",
    "To use knowledge distillation you need to use a new loss function. This function should combine the student and teacher loss.\n",
    "\n",
    "* <span style=\"color:blue\">Implement a <code>HardDistillationLoss</code> function, which combines the student and teacher loss and weighs them with 0.5 each.</span>\n",
    "* <span style=\"color:blue\">The function should use \"hard\" labels (see the <a href=\"https://arxiv.org/pdf/2012.12877\">paper</a> for hints).</span>\n",
    "* <span style=\"color:blue\">Use <code>CrossEntropyLoss</code>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fecf2f-7a25-4e33-b559-dbfc0c817534",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "* Use the existing implementations that where mentioned before, if you need guidance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1bb278-a02d-4b27-8c26-d39864d74230",
   "metadata": {},
   "source": [
    "### Task 3.3: Changing the ViT architecture to allow distillation\n",
    "The distillation procedure in the [paper](https://arxiv.org/pdf/2012.12877) works by introducing a distillation token, which plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher. Both tokens interact in the transformer through attention. In this task you should change the ViT architecture to integrate this token!\n",
    "\n",
    "\n",
    "* <span style=\"color:blue\">Add a distillation token to the <code>EmbedLayer</code>. This works the same way as the classification token. You can also initialize it as a <code>nn.Parameter</code> with <code>torch.zeros</code>. </span>\n",
    "* <span style=\"color:blue\">Think about what needs to be changed for the <code>pos_embedding</code>!</span>\n",
    "* <span style=\"color:blue\">Change the <code>Classifier</code> class to use the classification token <i>and</i> distillation token for prediction. <b>Add a linear layer</b> to project the distillation token (you dont need 2 layers and activation function here, just the linear layer is enough). During training, the <code>Classifier</code> should output the classification <b>and</b> the distillation projection, which will be used by the loss function from task 3.2. During inference, both outputs should be averaged. </span>\n",
    "* <span style=\"color:blue\">Name your new model <code>MyDeiT</code>. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a14bc3-7925-49a9-824c-b910fd297f52",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "* Use the existing implementations that where mentioned before, if you need guidance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f82dc-91b1-4772-b27c-3cdeab410850",
   "metadata": {},
   "source": [
    "### Task 3.4: Train your own DeiT model\n",
    "\n",
    "* <span style=\"color:blue\">Initialize a student model based on your new <code>MyDeiT</code> class. Use the same parameter configuration as before. </span>\n",
    "* <span style=\"color:blue\">Train the student model with the <code>HardDistillationLoss</code> you built in task 3.2!</span>\n",
    "* <span style=\"color:blue\">Your training loop might need to account for the second output (the distillation token) when making batch predictions during <code>train()</code> mode. </span>\n",
    "* <span style=\"color:blue\">Interpret your results!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62ae2b-cdb1-4404-94d8-7ef0d6d78587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
